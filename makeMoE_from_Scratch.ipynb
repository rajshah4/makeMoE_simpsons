{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rajshah4/makeMoE_simpsons/blob/main/makeMoE_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e9b80fc-12cf-41a9-a0de-354f678b412b",
     "showTitle": false,
     "title": ""
    },
    "id": "90vgVgmDkRJQ"
   },
   "source": [
    "# makeMoE: Sparse Mixture of Experts from Scratch\n",
    "\n",
    "## 1. Welcome and Learning Goals\n",
    "\n",
    "This notebook expands on Andrej Karpathy's original makemore tutorial by layering in a sparse Mixture of Experts (MoE) architecture. The walkthrough is designed as a learning companion: we build every component from first principles, keep the code hackable, and surface prompts you can explore on your own.\n",
    "\n",
    "**Learning goals**\n",
    "- Understand how character-level language models ingest and encode raw text data.\n",
    "- Reconstruct causal self-attention and see how it slots into a Transformer block.\n",
    "- Implement sparse gating, experts, and routing to form a Mixture of Experts layer.\n",
    "- Train the full model while tracking experiments and analysing generated samples.\n",
    "\n",
    "**How to use this notebook**\n",
    "1. Read the narrative in each section before running the code cells.\n",
    "2. Experiment with the \"Try it\" prompts to internalise the ideas.\n",
    "3. Keep notes on what changes improved or hurt performanceâ€”treat this notebook as a sandbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [1. Welcome and Learning Goals](#1-welcome-and-learning-goals)\n",
    "- [2. Project Setup](#2-project-setup)\n",
    "- [3. Data Acquisition and Tokenization](#3-data-acquisition-and-tokenization)\n",
    "- [4. Batching and Context Windows](#4-batching-and-context-windows)\n",
    "- [5. Self-Attention Foundations](#5-self-attention-foundations)\n",
    "- [6. Experts and Routing](#6-experts-and-routing)\n",
    "- [7. Building the Sparse MoE Block](#7-building-the-sparse-moe-block)\n",
    "- [8. Assembling the Language Model](#8-assembling-the-language-model)\n",
    "- [9. Training and Experiment Tracking](#9-training-and-experiment-tracking)\n",
    "- [10. Sampling and Evaluation](#10-sampling-and-evaluation)\n",
    "- [11. Further Experiments and Learning Paths](#11-further-experiments-and-learning-paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4a58a8-bd4c-40de-a4a9-95457842db0b",
     "showTitle": false,
     "title": ""
    },
    "id": "hywLNfb0kRJT"
   },
   "source": [
    "![mixture of experts overview](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/moe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Setup\n",
    "\n",
    "This section makes sure your environment is ready to run the notebook smoothly. We install dependencies, import core libraries, and optionally configure experiment tracking.\n",
    "\n",
    "> ðŸ’¡ **Try it:** If you are working offline or prefer a different tracker, comment out the Weights & Biases cell and replace it with your tool of choice (Aim, Comet, TensorBoard, â€¦).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b3daa3-3b3b-47af-b3e7-be95878f9e06",
     "showTitle": false,
     "title": ""
    },
    "id": "RQAnP6_RkRJU"
   },
   "outputs": [],
   "source": [
    "#Using Weights & Biases is entirely optional. I personally like to use W&B to track and log everything. You can pip install it easily.\n",
    "%pip install wandb weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Imports and Reproducibility\n",
    "\n",
    "We rely almost entirely on PyTorch. Setting a manual seed keeps runs comparable when you tweak hyperparameters.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Change the seed and see how early validation loss varies. Does model performance stabilize as you increase the number of training iterations?  What does that tell us about hte importance of the seed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1a3e38-8717-42ec-9bbc-71d3712c1c68",
     "showTitle": false,
     "title": ""
    },
    "id": "V521QQ_qkRJV"
   },
   "outputs": [],
   "source": [
    "#Import the necessary packages and set seed for reproducibility. For this notebook, pytorch is all you need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "#Optional\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optional: Configure Experiment Tracking\n",
    "\n",
    "Authenticate with Weights & Biases (or skip this cell if you prefer). For shared notebooks, set the API key through environment variables outside the notebook when possible.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Replace the hardcoded key with `wandb.login()` or a secrets manager integration if you want to share this notebook safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faf99ef2-39bb-46fc-b772-05d6d0482bbc",
     "showTitle": false,
     "title": ""
    },
    "id": "-4r_QNRRkRJV"
   },
   "source": [
    "## 3. Data Acquisition and Tokenization\n",
    "\n",
    "We will work with the Simpsons script dataset (a fun alternative to the original Shakespeare corpus). The next cells download the text, preview it, and build the integer encodings our model understands.\n",
    "\n",
    "**Steps ahead**\n",
    "1. Fetch the raw corpus.\n",
    "2. Inspect a sample to understand style and vocabulary.\n",
    "3. Build mappings between characters and integer ids.\n",
    "4. Encode the entire dataset and split into train/validation sets.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Swap in any other plain-text corpus. Update the download path and revisit the vocabulary sizeâ€”how does pop lyrics or code differ from TV dialogue?  \n",
    "Get shakespeare from here: #!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192e830a-762d-4573-9484-70a58deb1fec",
     "showTitle": false,
     "title": ""
    },
    "id": "3sPAL1AKkRJW"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('simpsons.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7181b7-f5e5-4ab5-bdd8-74c507c798ad",
     "showTitle": false,
     "title": ""
    },
    "id": "wNkF3RYLkRJX",
    "outputId": "671be677-be3d-49e0-d391-569559a99ae2"
   },
   "outputs": [],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68032e07-8625-4750-a340-bc8f4eed2458",
     "showTitle": false,
     "title": ""
    },
    "id": "AHIwr-yxkRJX",
    "outputId": "fa10ee0f-c8cb-4ba1-9fb1-79f9fbf336fe"
   },
   "outputs": [],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6995ad6-c9ac-4a21-9da0-ebbd3273c991",
     "showTitle": false,
     "title": ""
    },
    "id": "DHGayz7mkRJY",
    "outputId": "54b52e55-b6b9-4bdc-974a-5c48a43e1497"
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43002fa3-ffd3-416c-9aaf-0a03b19c7bc1",
     "showTitle": false,
     "title": ""
    },
    "id": "pzn11WJckRJY",
    "outputId": "cd7b3ff1-680b-4036-b3c8-b597a361aeb3"
   },
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4609fc4-09c7-4a39-8367-e9ee39d440ed",
     "showTitle": false,
     "title": ""
    },
    "id": "YbBGz0O2kRJY",
    "outputId": "49e94dde-6e1f-4a9e-a18b-9b8f1c51de86"
   },
   "outputs": [],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f7cb0d-02ff-42b0-92a5-a505dc3f8f25",
     "showTitle": false,
     "title": ""
    },
    "id": "hoLIeA7YkRJZ"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, the rest validation (val)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batching and Context Windows\n",
    "\n",
    "Transformers consume fixed-length context windows. These cells explore how we construct batches of contiguous characters and targets for next-token prediction.\n",
    "\n",
    "We are going to build the core training loop here so given the context (x), predict the next token (y).\n",
    "\n",
    "> ðŸ’¡ **Try it:** Experiment with larger `block_size` values to see how context length influences memory usage and validation loss. You may also try variable-length batching for a stretch goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b554ddf-50f4-441b-8acf-10b81a508b7e",
     "showTitle": false,
     "title": ""
    },
    "id": "VY55nr6EkRJZ",
    "outputId": "1a5685e9-66bb-40ed-f19a-f0cb09e8e51c"
   },
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ba4512-309d-4895-a908-2ef3efa317bc",
     "showTitle": false,
     "title": ""
    },
    "id": "5YbgrB9HkRJZ",
    "outputId": "05577a5f-10d5-495b-82e8-c3278b4fdea0"
   },
   "outputs": [],
   "source": [
    "# setup next token prediction \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf386bff-0f63-4358-82fc-6c7d02c37321",
     "showTitle": false,
     "title": ""
    },
    "id": "Oaxhage8kRJZ"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99acd85c-233f-4f2d-a062-028dbcde9960",
     "showTitle": false,
     "title": ""
    },
    "id": "HfpkIUNdkRJZ",
    "outputId": "1ad2bc10-2b4a-458b-a6ee-a169728ec90d"
   },
   "outputs": [],
   "source": [
    "# get some random starting positions\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46dc826-9f39-4aed-b2d2-f9ea401136de",
     "showTitle": false,
     "title": ""
    },
    "id": "faoGVPG3kRJa",
    "outputId": "dcf8b139-d8f8-4565-a962-62d094c5da9f"
   },
   "outputs": [],
   "source": [
    "# Build input (x) and target (y)\n",
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2886aedf-200e-40bd-9a9d-4658cf6c509b",
     "showTitle": false,
     "title": ""
    },
    "id": "hkllYFCPkRJa",
    "outputId": "9350713c-be0f-492a-c873-bd279226668d"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a486fc04-ed29-456f-918b-5f8395e455cb",
     "showTitle": false,
     "title": ""
    },
    "id": "tWrajECBkRJa"
   },
   "source": [
    "### 4.1 Visualising Autoregressive Behaviour\n",
    "\n",
    "This printout shows how each target token is conditioned on an ever-growing context. Inspecting it helps demystify how character-level models learn next-step predictions.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Modify `batch_size` or `block_size` and rerun the cell to see how the table changes. This is a quick way to debug your batching logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a86e10-ac37-4b92-8f18-775cd4853fdc",
     "showTitle": false,
     "title": ""
    },
    "id": "xjgtxxztkRJa",
    "outputId": "1be0ebc2-b22d-4136-e77d-ecdafded66d2"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde3273f-0519-4108-ba84-dfd99e020722",
     "showTitle": false,
     "title": ""
    },
    "id": "RlON_gNikRJa"
   },
   "source": [
    "## 5. Self-Attention Foundations\n",
    "\n",
    "Before we add experts, we revisit causal self-attention. You will see how queries, keys, values, and masking work together to respect autoregressive ordering.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Temporarily remove the causal mask (`tril`) and observe how the model cheats by attending to future tokensâ€”validation loss will drop suspiciously fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e435d0cf-1383-446a-9026-cd80b4266019",
     "showTitle": false,
     "title": ""
    },
    "id": "uBVWP40SkRJa"
   },
   "source": [
    "![scaled dot product self attention](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/self_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97660589-1719-48c0-ad6f-4f7c2888348a",
     "showTitle": false,
     "title": ""
    },
    "id": "i-jFgMntkRJa"
   },
   "source": [
    "The provided code demonstrates self-attention's mechanics and fundamental concepts, specifically focusing on the classic scaled dot product self-attention. In this variant, the query, key, and value matrices all originate from the same input sequence. To ensure the integrity of the autoregressive language generation process, particularly in a decoder-only model, the code implements masking. This masking technique is crucial as it obscures any information following the current token's position, thereby directing the model's attention to only the preceding parts of the sequence. Such an attention mechanism is known as causal self-attention. It's important to note that the Sparse Mixture of Experts model isn't restricted to decoder-only Transformer architectures. In fact, much of the significant work in this field, particularly that by Shazeer et al, revolves around the T5 architecture, which encompasses both encoder and decoder components in the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f82ca41-a301-4a92-aed9-ba7ac3a2bf88",
     "showTitle": false,
     "title": ""
    },
    "id": "lxMSgZWGkRJb",
    "outputId": "8d968403-a14f-4f3c-f2ce-9099a2b34f1e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch size, sequence length (time steps), number of channels (embedding dimension)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# wei:weights - attention scores (before softmax), shape (B, T, T)\n",
    "# Each [b, t1, t2] entry is the dot product between the query at position t1 and the key at position t2 for batch b\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# tril: lower-triangular matrix of shape (T, T), used to mask out future positions (causal masking)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Mask out (set to -inf) all positions in wei where tril == 0 (i.e., future positions)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Apply softmax to get normalized attention weights along the last dimension\n",
    "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "v = value(x) # (B, T, 16)\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) -> (B, T, 16)\n",
    "# The output from this final matrix product is subsequently passed through a linear layer as shown in the diagram above\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual diagram of what is happening\n",
    "```\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " Input â†’ â”‚ Linear (Q,K,V)â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”¬â”€â”¬â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚ â”‚ â”‚\n",
    "               Q K V\n",
    "               â”‚ â”‚ â”‚\n",
    "            â”Œâ”€â”€â–¼â”€â”´â”€â–¼â”€â”€â”\n",
    "            â”‚ Dot Prod â”‚  â†’ wei (B, T, T)\n",
    "            â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚ (mask + softmax)\n",
    "                â–¼\n",
    "          Attention Weights\n",
    "                â”‚\n",
    "                â–¼\n",
    "         Weighted Sum with V\n",
    "                â”‚\n",
    "                â–¼\n",
    "              Output (B, T, 16)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c278ec-19db-4c5d-b4a3-3bdc45c5a443",
     "showTitle": false,
     "title": ""
    },
    "id": "cA3iXggEkRJb"
   },
   "source": [
    "### 5.1 Modularising Attention Heads\n",
    "\n",
    "We encapsulate the single-head computation inside a `Head` module. This mirrors the design in PyTorch's official Transformer implementation.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Add dropout to the value projections or switch the activation in the attention weights to experiment with regularisation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608c6c9f-fb93-43ed-9580-5e782fd90d61",
     "showTitle": false,
     "title": ""
    },
    "id": "909nX3PHkRJb"
   },
   "outputs": [],
   "source": [
    "#Causal scaled dot product self-Attention Head\n",
    "# This code block defines the Head class for a single self-attention head, along with some hyperparameters.\n",
    "# This is based on the earlier attention head\n",
    "\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "head_size = 16\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8b31af-f45a-4066-8288-fb0d9c8e2aff",
     "showTitle": false,
     "title": ""
    },
    "id": "T3MoVK_WkRJb"
   },
   "outputs": [],
   "source": [
    "#Multi-Headed Self Attention\n",
    "# This class implements multi-head self-attention, a core component of the Transformer architecture.\n",
    "# It runs several attention \"heads\" in parallel, each with its own set of parameters, and then projects their concatenated outputs back to the original embedding size.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Create a list of independent attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Linear projection to combine the outputs of all heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout for regularization after projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run each head in parallel and concatenate their outputs along the last dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Project the concatenated outputs back to the embedding dimension and apply dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16267e9a-008b-46e3-82ce-2ae41396a1a1",
     "showTitle": false,
     "title": ""
    },
    "id": "T-w53_mSkRJb",
    "outputId": "052e4478-b284-43c7-924c-fcf13deea320"
   },
   "outputs": [],
   "source": [
    "# This code checks whether the MultiHeadAttention module returns an output with the same embedding size as its input.\n",
    "B, T, C = 4, 8, 64  # batch size, sequence length, embedding dimension\n",
    "x = torch.randn(B, T, C)  # create a random input tensor\n",
    "mha = MultiHeadAttention(num_heads=4, head_size=16)  # instantiate multi-head attention (4 heads Ã— 16 = 64)\n",
    "output = mha(x)\n",
    "print(output.shape)  # should print: torch.Size([4, 8, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f7ff128-7fe5-4a91-b9f2-208e2132e505",
     "showTitle": false,
     "title": ""
    },
    "id": "wNlJTtfhkRJb"
   },
   "source": [
    "## 6. Experts and Routing\n",
    "\n",
    "Sparse MoE layers replace the dense feed-forward block with a pool of experts. Only a subset runs per token, which gives us capacity without proportional compute.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Change the number of experts or their hidden size. How does capacity trade off with latency on your hardware?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e422a5-57c1-4b2f-b7b9-2757e109848a",
     "showTitle": false,
     "title": ""
    },
    "id": "zv3fGRpbkRJb"
   },
   "source": [
    "In the Sparse Mixture of Experts (MoE) architecture, the self-attention mechanism within each transformer block remains unchanged. However, a notable alteration occurs in the structure of each block: the standard feed-forward neural network is replaced with several sparsely activated feed-forward networks, known as experts. \"Sparse activation\" refers to the process where each token in the sequence is routed to only a limited number of these experts â€“ typically one or two â€“ out of the total pool available. This modification allows for specialized processing of different parts of the input data, enabling the model to handle a wider range of complexities efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe9fdcc-82eb-4047-9233-ad3cfe8759b1",
     "showTitle": false,
     "title": ""
    },
    "id": "7Kz0Y_P0kRJc"
   },
   "source": [
    "![experts](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/experts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f0f382-ab4a-45e1-9dce-27ae0d3da641",
     "showTitle": false,
     "title": ""
    },
    "id": "a-9CYWXgkRJc"
   },
   "outputs": [],
   "source": [
    "# Expert module: defines a single expert as a small feed-forward neural network (MLP)\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    An Expert is a simple MLP (Multi-Layer Perceptron) consisting of:\n",
    "    - A linear transformation to increase the embedding dimension (hidden layer)\n",
    "    - A non-linearity (ReLU activation)\n",
    "    - A linear transformation to project back to the original embedding dimension\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # The expert's network: input -> hidden (4x size) -> ReLU -> output -> Dropout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expands embedding dimension by 4x\n",
    "            nn.ReLU(),                      # Non-linear activation\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Projects back to original embedding size\n",
    "            nn.Dropout(dropout),             # Dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the expert's MLP\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7764385-26e9-4d75-9aa7-ce011023e24e",
     "showTitle": false,
     "title": ""
    },
    "id": "qderdEuykRJc"
   },
   "source": [
    "### 6.1 Top-k Gating Intuition\n",
    "\n",
    "The router scores each expert before selecting the top-k candidates per token. Keeping the walkthrough example grounded in small tensors makes the later implementation easier to follow.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Visualise the gating logits with a heatmap to check whether certain experts dominate. You can also try sampling experts proportionally rather than taking the strict top-k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fca4df-4c47-4e9a-98cd-08cf8ccf7726",
     "showTitle": false,
     "title": ""
    },
    "id": "VxJv5y44kRJc"
   },
   "source": [
    "![top k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/topk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e494b86-cdb2-4f2a-8824-5fa2ef4b2606",
     "showTitle": false,
     "title": ""
    },
    "id": "n6DuhY0DkRJc"
   },
   "source": [
    "The gating network, also known as the router, determines which expert network receives the output for each token from the multi-head attention. Let's consider a simple example: suppose there are 4 experts, and the token is to be routed to the top 2 experts. Initially, we input the token into the gating network through a linear layer. This layer projects the input tensor from a shape of (2, 4, 32) â€” representing (Batch size, Tokens, n_embed, where n_embed is the channel dimension of the input) â€” to a new shape of (2, 4, 4), which corresponds to (Batch size, Tokens, num_experts), where num_experts is the count of expert networks. Following this, we determine the top k=2 highest values and their respective indices along the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621916ff-2290-4e2f-9fd7-5181ed98d540",
     "showTitle": false,
     "title": ""
    },
    "id": "pNAuFDDvkRJc",
    "outputId": "a9d313d5-1dd1-4df8-9c9a-dc6d0b90d4ed"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Example: Understanding how top-k gating works in a Mixture of Experts (MoE) model\n",
    "\n",
    "# Number of experts in the MoE layer\n",
    "num_experts = 4\n",
    "\n",
    "# Number of experts to select (top-k) for each token\n",
    "top_k = 2\n",
    "\n",
    "# Embedding dimension of each token\n",
    "n_embed = 32\n",
    "\n",
    "# Simulate the output from a multi-head attention block\n",
    "# Shape: (batch_size=2, context_length=4, n_embed=32)\n",
    "mh_output = torch.randn(2, 4, n_embed)\n",
    "\n",
    "# Linear layer to produce logits for each expert from the token embeddings\n",
    "# Projects from n_embed to num_experts for each token\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts)  # nn.Linear(32, 4)\n",
    "\n",
    "# Compute the logits for each expert for every token\n",
    "logits = topkgate_linear(mh_output)\n",
    "\n",
    "# Select the top-k expert logits and their indices for each token\n",
    "# top_k_logits: the values of the top-k logits\n",
    "# top_k_indices: the indices of the top-k experts\n",
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
    "\n",
    "# Display the top-k logits and their corresponding expert indices\n",
    "top_k_logits, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f135ff7-0aa3-4b6d-ab5e-42399c48427b",
     "showTitle": false,
     "title": ""
    },
    "id": "EKwAyJxrkRJd"
   },
   "source": [
    "Obtain the sparse gating output by only keeping the top k values in their respective index along the last dimension. Fill the rest with '-inf' and pass through a softmax activation. This pushes '-inf' values to zero, makes the top two values more accentuated and sum to 1. This summation to 1 helps with the weighting of expert outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735e160a-ef1e-424d-b6d9-09f63ea99ec1",
     "showTitle": false,
     "title": ""
    },
    "id": "IiVejzOpkRJd",
    "outputId": "2ff61217-56f8-4835-d310-2169240fb4a1"
   },
   "outputs": [],
   "source": [
    "# Create a tensor of the same shape as logits, filled with -inf.\n",
    "# This will be used to mask out all but the top-k expert logits for each token.\n",
    "zeros = torch.full_like(logits, float('-inf'))  # full_like clones the shape and dtype of logits, filling with -inf.\n",
    "\n",
    "# Use scatter to place the top-k logits at their corresponding expert indices.\n",
    "# All other positions remain -inf, so only the top-k experts are \"active\" for each token.\n",
    "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
    "\n",
    "# Display the sparse logits tensor, where only the top-k expert positions have values, others are -inf.\n",
    "sparse_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9146e6f9-4eee-4a8b-8338-55072719ed59",
     "showTitle": false,
     "title": ""
    },
    "id": "HFgRxDF4kRJh",
    "outputId": "cde71bbf-ba67-4972-eef1-2fba48a106d5"
   },
   "outputs": [],
   "source": [
    "# Apply softmax to the sparse logits along the last dimension to obtain the gating output.\n",
    "# This converts the -inf-masked logits into a sparse probability distribution,\n",
    "# where only the top-k expert positions have nonzero values (sum to 1 for each token).\n",
    "gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "gating_output  # Display the resulting sparse gating output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe558b12-e443-4b62-9a85-c59120456352",
     "showTitle": false,
     "title": ""
    },
    "id": "dGJrq2uqkRJh"
   },
   "source": [
    "### 6.2 Noisy Top-k Routing\n",
    "\n",
    "Adding stochasticity to the router encourages load balancing: so different experts get a chance to learn before the model during training.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Tune the noise scale or swap in Gumbel noise. Monitor the router entropy during training to ensure the load is balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45516b59-d814-4853-a34e-d36aae9f04eb",
     "showTitle": false,
     "title": ""
    },
    "id": "TKp4DqwYkRJh"
   },
   "outputs": [],
   "source": [
    "# Define the Top-k Router module for selecting top-k experts per token\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embed (int): Dimensionality of input embeddings.\n",
    "            num_experts (int): Number of experts in the MoE layer.\n",
    "            top_k (int): Number of top experts to route each token to.\n",
    "        \"\"\"\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        # Linear layer to produce routing logits for each expert\n",
    "        self.linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mh_output (Tensor): Output tensor from the multi-head self-attention block,\n",
    "                                shape (..., n_embed).\n",
    "        Returns:\n",
    "            router_output (Tensor): Sparse softmax probabilities over experts,\n",
    "                                    only top-k per token are nonzero.\n",
    "            indices (Tensor): Indices of the top-k experts for each token.\n",
    "        \"\"\"\n",
    "        # Compute routing logits for each expert\n",
    "        logits = self.linear(mh_output)\n",
    "        # Select the top-k logits and their indices along the expert dimension\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        # Create a tensor filled with -inf to mask out non-top-k experts\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        # Scatter the top-k logits into their respective positions; others remain -inf\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        # Apply softmax to obtain sparse gating probabilities (only top-k nonzero)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c500844f-0866-4bbf-acef-c0c1d4979721",
     "showTitle": false,
     "title": ""
    },
    "id": "KjkouzwkkRJh",
    "outputId": "4e5cd861-4f75-4e98-c3cf-5697fb5f7451"
   },
   "outputs": [],
   "source": [
    "#Testing this out:\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 32\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices\n",
    "#And it works!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa02d0c-3688-4d01-811d-d0b2b851ab33",
     "showTitle": false,
     "title": ""
    },
    "id": "tAouN-GwkRJi"
   },
   "source": [
    "Althought the mixtral paper released recently does not make any mention of it, I believe Noisy top-k Gating is an important tool in training MoE models. Essentially, you don't want all the tokens to be sent to the same set of 'favored' experts. You want a fine balance of exploitation and exploration. For this purpose, to load balance, it is helpful to add standard normal noise to the logits from the gating linear layer. This makes training more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05b3306-b89f-4ebc-901b-f16398a925c2",
     "showTitle": false,
     "title": ""
    },
    "id": "aWeueE83kRJi"
   },
   "source": [
    "![noisy top-k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/noisytopkgating.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda4d805-373c-48f7-9037-da08fbc06e64",
     "showTitle": false,
     "title": ""
    },
    "id": "ZBrN-w3JkRJi"
   },
   "outputs": [],
   "source": [
    "#Changing the above to accomodate noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01a9d6b-fedb-427d-b0da-c3b2a75a8643",
     "showTitle": false,
     "title": ""
    },
    "id": "7Q6KcH9AkRJi",
    "outputId": "7aea5328-ba96-4c98-b550-042ecf440700"
   },
   "outputs": [],
   "source": [
    "#Testing this out, again:\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices\n",
    "#It works!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "076fa004-a165-42a7-b729-0bca8ad39418",
     "showTitle": false,
     "title": ""
    },
    "id": "XyKjpR-dkRJi"
   },
   "source": [
    "## 7. Building the Sparse MoE Block\n",
    "\n",
    "Now we bring routing and experts together. The `SparseMoE` module applies experts selectively and aggregates their outputs with gating weights.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Profile how many tokens each expert processes per batch. You can log the routing decisions to W&B for diagnostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6747b8de-0086-4cb0-8fbd-46ee95457eb9",
     "showTitle": false,
     "title": ""
    },
    "id": "UsRCy7i3kRJi"
   },
   "source": [
    "The primary aspect of this process involves the gating network's output. After acquiring these results, the top k values are selectively multiplied with the outputs from the corresponding top-k experts for a given token. This selective multiplication forms a weighted sum, which constitutes the SparseMoe block's output. The critical and challenging part of this process is to avoid unnecessary multiplications. It's essential to conduct forward passes only for the top_k experts and then compute this weighted sum. Performing forward passes for each expert would defeat the purpose of employing a sparse MoE, as it would no longer be sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6809b3f-4be9-4859-b39e-24fcdd6c8d86",
     "showTitle": false,
     "title": ""
    },
    "id": "7dDUHU_IkRJi"
   },
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        # Router determines which experts to use for each token\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        # List of expert networks (each is an MLP)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input tensor of shape (batch, seq_len, n_embed)\n",
    "        # Get gating weights and top-k expert indices from the router\n",
    "        gating_output, indices = self.router(x)\n",
    "        # Initialize output tensor (same shape as input)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Flatten input and gating output for easier indexing\n",
    "        flat_x = x.view(-1, x.size(-1))  # (batch*seq_len, n_embed)\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))  # (batch*seq_len, num_experts)\n",
    "\n",
    "        # Iterate over each expert\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for tokens where this expert is in the top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)  # (batch, seq_len)\n",
    "            flat_mask = expert_mask.view(-1)  # (batch*seq_len,)\n",
    "\n",
    "            # Only process tokens assigned to this expert\n",
    "            if flat_mask.any():\n",
    "                # Select inputs for this expert\n",
    "                expert_input = flat_x[flat_mask]  # (num_selected, n_embed)\n",
    "                # Forward pass through the expert\n",
    "                expert_output = expert(expert_input)  # (num_selected, n_embed)\n",
    "\n",
    "                # Get the corresponding gating scores for this expert\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)  # (num_selected, 1)\n",
    "                # Weight the expert outputs by the gating scores\n",
    "                weighted_output = expert_output * gating_scores  # (num_selected, n_embed)\n",
    "\n",
    "                # Add the weighted outputs to the final output tensor\n",
    "                # (scatter-add for all tokens assigned to this expert)\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        # Return the aggregated output from all experts\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06239630-0a1c-47c9-976c-7770f3d82e18",
     "showTitle": false,
     "title": ""
    },
    "id": "q8kDLI1ukRJj",
    "outputId": "69e7b330-cd47-4094-99c2-7928f8b60d0d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Demo: Testing SparseMoE Routing and Aggregation ---\n",
    "\n",
    "# Parameters for the test\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "dropout = 0.1\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "\n",
    "# Create a random input tensor simulating the output of a multi-head attention block\n",
    "mh_output = torch.randn(batch_size, seq_len, n_embd)\n",
    "\n",
    "# Instantiate the SparseMoE module\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "\n",
    "# Forward pass through the SparseMoE\n",
    "final_output = sparse_moe(mh_output)\n",
    "\n",
    "print(\"Input shape (batch, seq_len, n_embd):\", mh_output.shape)\n",
    "print(\"Output shape (should match input):\", final_output.shape)\n",
    "\n",
    "# This prints the sum of all values in the final output tensor from the SparseMoE layer.\n",
    "# If the routing and aggregation are working, this sum should be nonzero (since the experts are producing outputs and the gating is nontrivial).\n",
    "# A zero sum would indicate that either all gating weights are zero or the experts are not contributing, which would be a bug.\n",
    "print(\"Sum of output tensor (should be nonzero):\", final_output.sum().item())\n",
    "\n",
    "# --- Deeper Test: Check that only top-k experts are active per token ---\n",
    "\n",
    "# To do this, we can directly use the router to inspect the gating and indices\n",
    "with torch.no_grad():\n",
    "    gating_weights, topk_indices = sparse_moe.router(mh_output)\n",
    "    # gating_weights: (batch, seq_len, num_experts)\n",
    "    # topk_indices: (batch, seq_len, top_k)\n",
    "    print(\"\\nSample of top-k expert indices for first batch element:\")\n",
    "    print(topk_indices[0])  # shape: (seq_len, top_k)\n",
    "    print(\"Gating weights for first token (should sum to <= 1):\")\n",
    "    print(gating_weights[0, 0])\n",
    "    print(\"Sum of gating weights for first token:\", gating_weights[0, 0].sum().item())\n",
    "\n",
    "# --- Sanity check: If all experts are used (top_k = num_experts), output should be a dense mixture ---\n",
    "dense_moe = SparseMoE(n_embd, num_experts, num_experts)\n",
    "dense_output = dense_moe(mh_output)\n",
    "print(\"\\nDense MoE output shape:\", dense_output.shape)\n",
    "\n",
    "# This line computes the L2 norm (Euclidean distance) between the outputs of the sparse MoE (which only routes each token to its top-k experts)\n",
    "# and the dense MoE (which routes each token to all experts, i.e., top_k = num_experts).\n",
    "# A nonzero difference shows that the sparse routing is actually selecting a subset of experts, not just acting like a dense mixture.\n",
    "print(\"Difference between sparse and dense output (L2 norm):\", (final_output - dense_output).norm().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7476ca07-a315-4108-aa77-46173a703ca2",
     "showTitle": false,
     "title": ""
    },
    "id": "l7GoxCi2kRJj"
   },
   "source": [
    "### 7.1 Weighted Expert Aggregation\n",
    "\n",
    "Remember that the gating weights are just as important as the expert outputsâ€”they determine how much each expert contributes to the final representation.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Try alternative combination strategies such as residual mixing or learnable temperature on the softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99b5dce-301e-4380-8263-b5cfb4136ab2",
     "showTitle": false,
     "title": ""
    },
    "id": "e9a_oQ2akRJj"
   },
   "source": [
    "![sparse MoE](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/sparseMoEfinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5f3be4-f155-4d6c-bcbd-2f88a088261f",
     "showTitle": false,
     "title": ""
    },
    "id": "vMZCda7dkRJj"
   },
   "source": [
    "## 8. Assembling the Language Model\n",
    "\n",
    "With the sparse MoE block ready, we can stack Transformer blocks, add embeddings, and define the language model head.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Compare this sparse architecture with a dense baseline by swapping the MoE block for a standard MLP. Track parameters vs. performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eaf71cd-c77e-40c7-b5be-e364e91685cf",
     "showTitle": false,
     "title": ""
    },
    "id": "f8yczkFHkRJj"
   },
   "outputs": [],
   "source": [
    "#First defining hyperparameters and boiler plate code. Imports and data preparation code is repeated for convenience\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 400\n",
    "head_size = 16\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('simpsons.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1180f7-5004-4425-87fe-9a81a17b9024",
     "showTitle": false,
     "title": ""
    },
    "id": "QfxJ6B2fkRJj"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "#Multi-Headed Self Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03611a92-aaa2-4e0d-9755-cba56f96c794",
     "showTitle": false,
     "title": ""
    },
    "id": "y35jVCZYkRJk"
   },
   "outputs": [],
   "source": [
    "#Expert module\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "#noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n",
    "\n",
    "#Now create the sparse mixture of experts module\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdff2bb-092f-41c8-9a33-c84e6f8d6633",
     "showTitle": false,
     "title": ""
    },
    "id": "jGiuRsSgkRJk"
   },
   "outputs": [],
   "source": [
    "#First create a self attention + mixture of experts block, that may be repeated several number of times\n",
    "#Copy pasting key architecture variables for clarity\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
    "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d32a276-d0cc-4808-90d7-62441771af44",
     "showTitle": false,
     "title": ""
    },
    "id": "RpyZBA71kRJk"
   },
   "outputs": [],
   "source": [
    "#Finally putting it all together to crease a sparse mixture of experts language model\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head, num_experts=num_experts,top_k=top_k) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622ba7ce-3f20-4820-8982-93f3d3b7be09",
     "showTitle": false,
     "title": ""
    },
    "id": "bBzQXmfykRJk"
   },
   "source": [
    "### 8.1 Weight Initialisation Notes\n",
    "\n",
    "We use Kaiming (He) initialization by default for the weights in our expert networks because these networks use ReLU activations. Kaiming initialization is specifically designed to maintain stable signal variance through layers with ReLU, helping gradients flow well and preventing vanishing/exploding issues during training.\n",
    " \n",
    "If you use a different activation function (such as Tanh or GELU), you might want to use a different initialization scheme, like Xavier (Glorot) initialization, which is better suited for activations that are symmetric around zero. The choice of initialization can have a significant impact on training stability and convergence speed, so it's a good idea to match your initialization to your activation function.\n",
    " \n",
    "You can experiment with per-layer or per-module initialization strategies. For example, you might want to use a smaller variance for the router's weights to help stabilize routing decisions early in training, while keeping Kaiming initialization for the expert MLPs.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Implement per-layer initialisation schemesâ€”e.g. try smaller variance for the router to stabilise early training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d3c057-08ee-4c1b-8013-6a88b2eadac5",
     "showTitle": false,
     "title": ""
    },
    "id": "guGaJqHbkRJk"
   },
   "outputs": [],
   "source": [
    "\n",
    "def kaiming_init_weights(m):\n",
    "    if isinstance (m, (nn.Linear)):\n",
    "        init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4d9525-8405-4a51-adda-661aba004e57",
     "showTitle": false,
     "title": ""
    },
    "id": "nJGGkXz4kRJl",
    "outputId": "8518ec23-caa0-4167-88c6-65a7c905743a"
   },
   "outputs": [],
   "source": [
    "model = SparseMoELanguageModel()\n",
    "model.apply(kaiming_init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6adf1d04-e668-4d14-b691-161ea4e4dccf",
     "showTitle": false,
     "title": ""
    },
    "id": "_cb1-L8ckRJl"
   },
   "source": [
    "## 9. Training and Experiment Tracking\n",
    "\n",
    "Training loops are more fun when you can compare runs. This section logs hyperparameters, losses, checkpoints, and generated samples to Weights & Biases (with a graceful fallback if you prefer to stay offline).\n",
    "\n",
    "> ðŸ’¡ **Try it:** Capture additional metricsâ€”router entropy, number of tokens per expert, gradient normsâ€”to deepen your understanding of training dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8968247-0d7b-4460-b96b-06743b31c55d",
     "showTitle": false,
     "title": ""
    },
    "id": "WTG1Fv4SkRJl",
    "outputId": "38318015-63a9-4959-cfe0-cab305625f49"
   },
   "outputs": [],
   "source": [
    "#Using Weights & Biases\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define generation checkpoints\n",
    "generation_steps = [0,100,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "\n",
    "# Simple local logging when W&B is not available\n",
    "local_logs = []\n",
    "\n",
    "# Function to generate text samples\n",
    "def generate_samples(model, step, num_samples=3, max_tokens=200):\n",
    "    \"\"\"Generate text samples and save them\"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Start with a random character from the vocabulary\n",
    "            context = torch.randint(0, vocab_size, (1, 1), device=device)\n",
    "            generated = model.generate(context, max_new_tokens=max_tokens)\n",
    "            text = decode(generated[0].tolist())\n",
    "            samples.append(text)\n",
    "    \n",
    "    model.train()\n",
    "    return samples\n",
    "\n",
    "# Function to save model checkpoint\n",
    "def save_model_checkpoint(model, step, optimizer):\n",
    "    \"\"\"Save model checkpoint as W&B artifact\"\"\"\n",
    "    checkpoint_path = f\"model_checkpoint_step_{step}.pt\"\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Create W&B artifact if W&B is available\n",
    "    if use_wandb:\n",
    "        try:\n",
    "            artifact = wandb.Artifact(f\"model_checkpoint_step_{step}\", type=\"model\")\n",
    "            artifact.add_file(checkpoint_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(f\"âœ… Model checkpoint uploaded to W&B: {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  W&B artifact upload failed: {e}\")\n",
    "            print(f\"âœ… Model checkpoint saved locally: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"âœ… Model checkpoint saved locally: {checkpoint_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "# Initialize W&B with error handling\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=\"makeMoE\",\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"block_size\": block_size,\n",
    "            \"max_iters\": max_iters,\n",
    "            \"eval_interval\": eval_interval,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"device\": device,\n",
    "            \"eval_iters\": eval_iters,\n",
    "            \"dropout\": dropout,\n",
    "            \"num_experts\": num_experts,\n",
    "            \"top_k\": top_k,\n",
    "            \"generation_steps\": generation_steps\n",
    "        },\n",
    "        # Disable automatic code logging to avoid connection issues\n",
    "        settings=wandb.Settings(\n",
    "            _disable_stats=True,\n",
    "            _disable_meta=True,\n",
    "            code_dir=None\n",
    "        )\n",
    "    )\n",
    "    print(\"âœ… W&B initialized successfully!\")\n",
    "    use_wandb = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  W&B initialization failed: {e}\")\n",
    "    print(\"Continuing without W&B logging...\")\n",
    "    use_wandb = False\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # Log metrics to W&B\n",
    "        if use_wandb:\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": float(losses['train']),\n",
    "                    \"val_loss\": float(losses['val']),\n",
    "                    \"step\": iter\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  W&B logging failed: {e}\")\n",
    "        else:\n",
    "            # Local logging fallback\n",
    "            local_logs.append({\n",
    "                \"step\": iter,\n",
    "                \"train_loss\": float(losses['train']),\n",
    "                \"val_loss\": float(losses['val'])\n",
    "            })\n",
    "\n",
    "    # Generate samples and save model at specified steps\n",
    "    if iter in generation_steps:\n",
    "        print(f\"\\n=== Generating samples at step {iter} ===\")\n",
    "        samples = generate_samples(model, iter)\n",
    "        \n",
    "        # Log samples to W&B\n",
    "        for i, sample in enumerate(samples):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(sample[:500] + \"...\" if len(sample) > 500 else sample)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Log to W&B as text\n",
    "            if use_wandb:\n",
    "                try:\n",
    "                    wandb.log({\n",
    "                        f\"generation_step_{iter}_sample_{i+1}\": wandb.Html(f\"<pre>{sample}</pre>\"),\n",
    "                        \"step\": iter\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  W&B generation logging failed: {e}\")\n",
    "            else:\n",
    "                # Local logging fallback\n",
    "                local_logs.append({\n",
    "                    \"type\": \"generation\",\n",
    "                    \"step\": iter,\n",
    "                    \"sample\": i+1,\n",
    "                    \"text\": sample\n",
    "                })\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = save_model_checkpoint(model, iter, optimizer)\n",
    "        print(f\"Model checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save final model\n",
    "print(\"\\n=== Saving final model ===\")\n",
    "final_samples = generate_samples(model, max_iters, num_samples=5, max_tokens=300)\n",
    "for i, sample in enumerate(final_samples):\n",
    "    print(f\"Final Sample {i+1}:\")\n",
    "    print(sample[:500] + \"...\" if len(sample) > 500 else sample)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if use_wandb:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                f\"final_generation_sample_{i+1}\": wandb.Html(f\"<pre>{sample}</pre>\"),\n",
    "                \"step\": max_iters\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  W&B final generation logging failed: {e}\")\n",
    "\n",
    "# Save final model checkpoint\n",
    "final_checkpoint = save_model_checkpoint(model, max_iters, optimizer)\n",
    "print(f\"Final model checkpoint saved: {final_checkpoint}\")\n",
    "\n",
    "# Save local logs if W&B was not available\n",
    "if not use_wandb and local_logs:\n",
    "    import json\n",
    "    with open(\"training_logs.json\", \"w\") as f:\n",
    "        json.dump(local_logs, f, indent=2)\n",
    "    print(\"âœ… Local training logs saved to training_logs.json\")\n",
    "\n",
    "# Finish the W&B run\n",
    "if use_wandb:\n",
    "    try:\n",
    "        wandb.finish()\n",
    "        print(\"âœ… W&B run finished successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  W&B finish failed: {e}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Training completed without W&B logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create a dataset of all generations for easy access\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_generations_dataset():\n",
    "    \"\"\"Create a dataset containing all generations from training\"\"\"\n",
    "    \n",
    "    # This would collect all the generations that were logged during training\n",
    "    # In a real scenario, you might want to store these in a more structured way\n",
    "    \n",
    "    generations_data = {\n",
    "        \"model_info\": {\n",
    "            \"architecture\": \"SparseMoE\",\n",
    "            \"num_experts\": num_experts,\n",
    "            \"top_k\": top_k,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"block_size\": block_size,\n",
    "            \"n_embed\": n_embed,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_layer\": n_layer\n",
    "        },\n",
    "        \"training_info\": {\n",
    "            \"max_iters\": max_iters,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"generation_steps\": generation_steps\n",
    "        },\n",
    "        \"generations\": {\n",
    "            # This would be populated with actual generations from training\n",
    "            # For now, it's a placeholder structure\n",
    "            \"step_0\": \"Generated at step 0 - random initialization\",\n",
    "            \"step_100\": \"Generated at step 100 - early training\",\n",
    "            \"step_500\": \"Generated at step 500 - learning patterns\",\n",
    "            \"step_1000\": \"Generated at step 1000 - improving coherence\",\n",
    "            \"step_2000\": \"Generated at step 2000 - better structure\",\n",
    "            \"step_3000\": \"Generated at step 3000 - final training\",\n",
    "            \"step_4000\": \"Generated at step 4000 - better structure\",\n",
    "            \"step_5000\": \"Generated at step 5000 - better structure\",\n",
    "            \"step_6000\": \"Generated at step 6000 - better structure\",\n",
    "            \"step_7000\": \"Generated at step 7000 - better structure\",\n",
    "            \"step_8000\": \"Generated at step 8000 - better structure\",\n",
    "            \"step_9000\": \"Generated at step 9000 - better structure\",\n",
    "            \"step_10000\": \"Generated at step 10000 - better structure\",\n",
    "            \"final\": \"Final generations after full training\"\n",
    "        },\n",
    "        \"created_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save as JSON file\n",
    "    with open(\"generations_dataset.json\", \"w\") as f:\n",
    "        json.dump(generations_data, f, indent=2)\n",
    "    \n",
    "    # Also save as W&B artifact if W&B is available\n",
    "    try:\n",
    "        if 'use_wandb' in globals() and use_wandb:\n",
    "            artifact = wandb.Artifact(\"generations_dataset\", type=\"dataset\")\n",
    "            artifact.add_file(\"generations_dataset.json\")\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(\"âœ… Dataset uploaded to W&B!\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸  Dataset saved locally (W&B not available)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  W&B dataset upload failed: {e}\")\n",
    "        print(\"âœ… Dataset saved locally\")\n",
    "    \n",
    "    print(\"Generations dataset created and saved!\")\n",
    "    return generations_data\n",
    "\n",
    "# Uncomment the line below to create the dataset\n",
    "# generations_dataset = create_generations_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sampling and Evaluation\n",
    "\n",
    "Periodic sampling shows how the model's voice evolves. We capture generations at multiple training steps, archive checkpoints, and create a lightweight dataset of outputs.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Add qualitative metricsâ€”perplexity on held-out text, distinct-ngrams, or sentiment scores if you work with other corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed96085-c292-4624-a2cc-be8aad38df79",
     "showTitle": false,
     "title": ""
    },
    "id": "jFBVfgfekRJl"
   },
   "source": [
    "### 10.1 Monitoring Loss Curves\n",
    "\n",
    "Validation loss is your early warning signal. Watch for divergence between train and validation to decide when to stop training or adjust regularisation.\n",
    "\n",
    "> ðŸ’¡ **Try it:** Plot additional diagnostics such as learning-rate warmup, or compare multiple runs within the W&B UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa6e4c4-c688-4985-a3b8-e2af1f771e54",
     "showTitle": false,
     "title": ""
    },
    "id": "W4yshpXMkRJl",
    "outputId": "e69af1c7-2e2f-475c-eeba-b20dec8532c2"
   },
   "outputs": [],
   "source": [
    "# generate from the model. Not great. Not too bad either\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Further Experiments and Learning Paths\n",
    "\n",
    "Ready to explore further? Here are some directions:\n",
    "- **Routing research:** Implement auxiliary load-balancing losses from the Switch Transformer or Mixtral papers.\n",
    "- **Curriculum learning:** Start with shorter sequences before ramping up context length.\n",
    "- **Tokenizer swaps:** Replace the character tokenizer with Byte Pair Encoding (BPE) to model longer-range structure.\n",
    "- **Inference tricks:** Experiment with nucleus sampling, temperature schedules, or expert pruning for faster inference.\n",
    "- **Visualisation:** Log attention maps and expert selections to better understand what each expert specialises in.\n",
    "\n",
    "If you make interesting discoveries, consider packaging them as pull requests or blog postsâ€”teaching others is the fastest way to solidify your understanding.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "makeMoE_from_Scratch",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
